---
title: "abstract explore"
author: "Kirk Gosik"
date: "3/31/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pubmed.mineR)
library(rentrez)
library(tm)
library(tidyverse)
library(tidytext)
library(widyr)
library(magrittr)
library(igraph)
library(networkD3)
library(foreach)
library(doParallel)
library(stringdist)
rm(list = ls()); gc(reset = TRUE)

```

## Helper Functions

```{r helpers}
## helper functions ####

simpleCap <- function(x) {
  s <- strsplit(x, " ")[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2),
        sep="", collapse=" ")
}

suggest_word <- function( word, list ) {
  
  list[which.min(stringdist(
    stemDocument(word), 
    stemDocument(list), method = "lv"))]
  
}

remove_dups <- function( word_list ){
  i <- 1
  while( i <= length(word_list) ) {
    
    remove_idx <- -which(stemDocument(tolower(word_list[i])) == stemDocument(tolower(word_list)))[-1]
    
    if( length(remove_idx) > 0 ) word_list <- word_list[remove_idx]
    i <- i + 1
  }
    
  word_list
}


slide_windows <- function(tbl, doc_var, window_size) {
  # each word gets a skipgram (window_size words) starting on the first
  # e.g. skipgram 1 starts on word 1, skipgram 2 starts on word 2
  
  each_total <- tbl %>% 
    group_by(!!doc_var) %>% 
    mutate(doc_total = n(),
           each_total = pmin(doc_total, window_size, na.rm = TRUE)) %>%
    pull(each_total)
  
  rle_each <- rle(each_total)
  counts <- rle_each[["lengths"]]
  counts[rle_each$values != window_size] <- 1
  
  # each word get a skipgram window, starting on the first
  # account for documents shorter than window
  id_counts <- rep(rle_each$values, counts)
  window_id <- rep(seq_along(id_counts), id_counts)
  
  
  # within each skipgram, there are window_size many offsets
  indexer <- (seq_along(rle_each[["values"]]) - 1) %>%
    map2(rle_each[["values"]] - 1,
         ~ seq.int(.x, .x + .y)) %>% 
    map2(counts, ~ rep(.x, .y)) %>%
    flatten_int() +
    window_id
  
  tbl[indexer, ] %>%
    bind_cols(data_frame(window_id)) %>%
    group_by(window_id) %>%
    filter(n_distinct(!!doc_var) == 1) %>%
    ungroup
}


nearest_synonyms <- function(df, token) {
  df %>%
    widely(~ . %*% (.[token, ]), sort = TRUE)(item1, dimension, value) %>%
    select(-item2)
}


analogy <- function(df, token1, token2, token3) {
  df %>%
    widely(~ . %*% (.[token1, ] - .[token2, ] + .[token3, ]), sort = TRUE)(item1, dimension, value) %>%
    select(-item2)
  
}
```


## Abstracts

```{r pressure, echo=FALSE}
# first 400 abstracts
# abstracts <- readabs("pubmed_result_crohns_search1.txt")
abstracts <- readabs("data/pubmed_result_ards.txt")
pmids <- abstracts@PMID[1:200]


cl <- makeCluster(4)
registerDoParallel(cl)

pubmed_list <- foreach( i = pmids, .export = "pubtator_function" ) %dopar% {
  pubtator_function(i)
}

stopCluster(cl)
```





## Text Mining

```{r}

source("src/TextMining_Analysis_Function.R")

ards <- readabs("data/pubmed_result_ards.txt")

ards_corpus <- VCorpus(VectorSource(ards@Abstract))

TM.Analysis(corpus = ards_corpus)
```


```{r}
## more stop words
data("stop_words")
stop_words <- rbind(stop_words, c("article", "mine"))
stop_words <- rbind(stop_words, c("publisher", "mine"))
stop_words <- rbind(stop_words, c("report", "mine"))
stop_words <- rbind(stop_words, c("doi", "mine"))
```


```{r}
ards <- readabs("data/pubmed_result_ards.txt")

## creating crohns tibble
ards_tibble <- tibble(PMID = ards@PMID,
                        text = ards@Abstract) %>%
  mutate(text = coalesce(text),
         text = str_replace_all(text, "&#x27;|&quot;|&#x2F;", "'"),   ## weird encoding
         text = str_replace_all(text, "<a(.*?)>", " "),               ## links 
         text = str_replace_all(text, "&gt;|&lt;|&amp;", " "),        ## html yuck
         text = str_replace_all(text, "&#[:digit:]+;", " "),          ## html yuck
         text = str_replace_all(text, "<[^>]*>", " "),                ## mmmmm, more html yuck
         text = str_replace_all(text, "'s", ""),
         text = str_replace_all(text, "20[0-9]{2}", ""),
         text = tolower(text))
         # text = str_replace_all(text, "inflammatory bowel disease", "ibd"),
         # text = str_replace_all(text, "ulcerative colitis" = "uc"),
         # text = str_replace_all(text, "crohn disease", "cd"))      ## accronyms

```


```{r}
## creating pmi of words in a 8 word window
tidy_pmi <- ards_tibble %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  add_count(word) %>%
  filter(n >= 20) %>%
  select(-n) %>%
  slide_windows(quo(PMID), 8) %>%
  pairwise_pmi(word, window_id)


tidy_word_vectors <- tidy_pmi %>%
  widely_svd(item1, item2, pmi, nv = 256, maxit = 1000)


tidy_word_vectors %>%
  nearest_synonyms("ards")

tidy_word_vectors %>%
  nearest_synonyms("sars")

tidy_word_vectors %>%
  analogy("ards", "lung", "sars")
```



```{r}
tidy_word_vectors %>%
  filter(dimension <= 12) %>%
  group_by(dimension) %>%
  top_n(12, abs(value)) %>%
  ungroup %>%
  mutate(item1 = reorder(item1, value)) %>%
  group_by(dimension, item1) %>%
  arrange(desc(value)) %>%
  ungroup %>%
  mutate(item1 = factor(paste(item1, dimension, sep = "__"), 
                        levels = rev(paste(item1, dimension, sep = "__"))),
         dimension = factor(paste0("Dimension ", dimension),
                            levels = paste0("Dimension ", as.factor(1:24)))) %>%
  ggplot(aes(item1, value, fill = dimension)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~dimension, scales = "free_y", ncol = 4) +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  coord_flip() +
  labs(x = NULL, y = "Value",
       title = "First 12 principal components of the ARDS corpus",
       subtitle = "Top words contributing to the components that explain the most variation")

```

